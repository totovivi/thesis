\documentclass[letterpaper, 10.5 pt, conference]{ieeeconf}
\IEEEoverridecommandlockouts
\overrideIEEEmargins

\usepackage{bm}
\usepackage{bbm}
\usepackage{algorithm,algpseudocode}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath}

\newcommand{\B}[1]{\mathbf{#1}}

\title{\LARGE \bf
Automated consistent course of actions in a stochastic environment
}

\author{Thomas Vicente$^{1}$% <-this % stops a space
\thanks{*Candidate for the Master in Data Science at the Barcelona Graduate School of Economics.}% <-this % stops a space
}

\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}

\begin{abstract}

I propose an algorithm allowing the automated learning of the best course of actions in a stochastic environment. It notably includes a signal function that should allow more flexible implementations than current approximation-based reinforcement learning algorithms. This function triggers the computation of discounted action values for each learning episode. The paper contains an application, a tic-tac-toe robot player recognizing underlying games from photographs via a neural network.

\end{abstract}

\section*{Acknowledgment}

I would like to thank my advisor, Professor Gábor Lugosi, for his help and for making himself available. I would also like to thank Gergely Neu, post-doctoral fellow at the Artificial Intelligence Research Group from the Pompeu Fabra University, for sharing his knowledge.

\section{Introduction}

A complete exercise would be to build an autonomous entity that is progressively able to take long term-oriented decisions based on information coming from real world sensors. This entity needs two abilities: learning from its own actions and recognizing useful patterns in the current state. The first ability typically corresponds to algorithms from the reinforcement learning (RL) field. The second ability requires regressor or classifier function that are able to handle the complexity of stochastic state's patterns. 

\subsection{Learning by action}

RL emerges easily from other Machine Learning techniques as the training data is generated thanks to the actions of the learning entity. Thus, the \textit{learner} uses training information that evaluates the actions taken rather than instructs by giving correct actions are faced repeatedly with a choice among $n$ different options, or actions. Such setting has its roots in the \textit{n-armed bandit problem} where after each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the selected action. The objective is to maximize the expected total reward over all the episodes. The learner evolves in a particular state and records what actions lead to a maximal gain. In most cases the goal is to maximize a long term gain by taking a correct course of actions. These policies can be more or less explicit. I our case, the proposed algorithm would be qualified as \textit{off-policy}.

An RL learner is thus particularly dependent on a state signal that succeeds in retaining all relevant information, a \textit{Markov} state. For example, a checkers position (the current configuration of all the pieces on the board) would serve as a Markov state because it summarizes everything important about the complete sequence of positions that led to it. Much of the information about the sequence is lost, but all that really matters for the future of the game is retained. 

A reinforcement learning task that satisfies the Markov property is called a Markov Decision Process (MDP). If the state and action spaces are finite, it is possible the MDP framework notably formalizes the probability that a state occurs given a previous state an action, and the probability of reward after the new state occurred.

\subsection{Pattern recognition}

Equipping the learner with an approximation function enables it to evaluate an infinite set of states but also a finite set of underlying states having an infinite set of representations. In Figure~\ref{fig:surf}, I illustrate the fact that humans are able to recognize the same underlying situation and take the correct course of actions.

\begin{figure}
\begin{center}
\includegraphics[scale=.35]{"surf"}
\caption{Waves are a stochastic process but, with training, we are able to act consistently for a new wave}
\label{fig:surf}
\end{center}
\end{figure}

In a similar manner, we would like our learner to overcome the imperfect information it receives and learn how to behave in the long term to maximize its gain. This is made possible notably by regressor and classifier functions that take particularly well advantage of naturally generated data such as images and sounds.

A modified version of the classical tic-tac-toe robot will provide a good example for the principle we seek to study.

\section{Literature review}

The proposed algorithm posseses similarities with the concepts that we are presenting in this section. We also are going to show that, to my knowledge, it is a novel approach in approximation-based RL. This would notably be due to the algorithm's minimal requirements for control of the states' values.

A close classical RL technique to the proposed algorithm, is the the State-Action-Reward-State-Action algorithm (SARSA). It uses approximations to update the value of a whole set of actions, a policy. SARSA is well suited when the value of each action can be monitored. It notably update the parameters of the approximation function after each movement thanks to a change in value from $t-1$ to $t$, known as a temporal difference. The update includes a gradient term, compute from the difference between the state's value approximation and the real state's value. Our algorithm however does not suppose the learning of a whole policy.

In that sense, the recent Deep Q networks (DQN) are closer to what is proposed. This off-policy algorithm uses neural network approximations of the states' value to decide on an action. And as the learner makes better moves, the neural network is fed with more useful information. And thanks to its \textit{experience replay}, when training the network, random mini-batches from the replay memory are used instead of the most recent transition. Such efficient implementation makes the DQN very adaptive to different tasks, as demonstrated for the learning of multiple arcade games while using the same network architecture. I also assume such memory device for the proposed algorithm. But I try to have a more flexible approach than this algorithm, which supposes the constant monitoring of the states' values. 

This is done thanks to a mechanism that is close to the True Online Temporal Difference Learning from Seijen et al.. The authors introduced a formal way to assign values in a backward manner to each state of the episode, when the latter ends. This is done in a discounted way so states farther to the end of an episode get a value with a smaller magnitude. In the proposed algorithm, however, as mentioned earlier, we want to avoid using a permanent control of the states' values during the learning phase. It then differs from the mentioned paper as we cannot update our parameters via traces. Instead, the improving estimation of the new states' value is done exclusively via the approximation function.

Hopefully the proposed algorithm offers a greater flexibility than the previous litterature. Removing the assumption that the state's values are permanently monitored during the training phase is made possible by the implementation of a \textit{signal function} coupled with a \textit{discount function}. The role of the first function is to detect an objective signal thanks to a deterministic function or a well tuned sensor. When triggered, it assigns a gain or loss-related value to the final state of the episode. The second function will allow to assign values to each state of a particular episode. This mechanism is supposed to allow a practical use of the algorithm when evaluating non-idealized states - not only video games screen shots - involving stochastic perturbations that are for instance recorded via a visual sensor. 

\section{Algorithm}

\subsection{Description}

We can summarise our setting as a decision process possibly lacking the Markov property. It has the form
$$\langle \mathcal{X},\Omega, P(\B{x_c},\B{x_{t+1}}), \B{y},s(\B{x_c}),d(\gamma, \Delta, \delta, s(\B{x_c})) \rangle$$
where $\mathcal{X}$ is a set of finite underlying states, $\Omega$ is the finite set of possible actions, $P(\B{x_c})$ is the probability that the process moves to a new state $\B{x_{t+1}}$ potentially depending on numerous previous states in the episode, $s()$ is the signal function and d() is a the discount function.

I am going to describe more details and chronologically what happens during an episode of the algorithm in action. Note that all the objects containing $X$ or $x$ in their names are $m$-dimensional objects, where $m$ is the number of features representing the considered states.

The learner observes a new stochastic state $\B{x_t}(p_t)$ - note that this stochastic state can be affected by previous moves from our learner. It then simulates actions within the finite set of possible actions $\Omega(\B{x_t})$. $\Omega(\B{x_t})$ can be set deterministically. For instance, in the context of a game, there is a well determined set of actions that a human can tune in advance. It also can be a subset of an infinite set of actions. For instance, a mobile robot can explore the real 3D world in almost infinite ways. In that case, the set of possible actions has to be restricted somehow, possibly by the time allowed to go in particular direction.

All of these simulated actions are stored in a temporary matrix $\B{X'}$. Each observation in this matrix is going to be evaluated by the a approximation function $f(\B{w},\B{X'})$. This function can be a regressor or a classifier which weights $w$ are randomly initialized before the first episode. It is going to return approximations of the current state's value. This value is proportional to the learner's closeness to its objective. Among the simulated states, we will then return the chosen $\B{x}$, $\B{x_c}$. With probability $1-\epsilon$ it corresponds to the state which has the highest approximated valu and with probability $\epsilon$, it will be a randomly chosen state from $\B{X'}$. If both cases, we append the returned state to the training set $\B{X}$.

The learner possesses a function $s(\B{x_c})$. It returns different values when a particular signal in the state generated by the learner's last move, $\B{x_c}$, is sensed. The value is positive if a \textit{gain} is sensed, and negative value if a \textit{loss} is sensed. At least one value should be potentially triggered. A typical example for $[gain;loss]$ would be respectively $[1;-1]$. While this signal is off, the learner counts the number of actions since the signal was last triggered. When the signal is triggered, the current episode is over. Each action of the episode receives a value given by the discount function $d(\gamma, \Delta, \delta, s(\B{x_c}))$ where $\gamma$ is a discount rate, $\Delta$ is the number of actions in the episode and $\delta$ is the index of the action in the episode. For instance, the last action in the episode has $\delta=\Delta$. The function should insure that actions that are closer in time from the triggered signal get values that are closer from $s(\B{x_c})$. Each value of the episode are then appended to the training target vector $\B{y}$. 

We then get a training sample on which are going to apply our approximation function $f(\B{w}, \B{X})$ which this time minimizes the loss $l(f(\B{w}, \B{X}),\B{y})$. From this process, we are going to retain set of parameters corresponding to the minimal loss to update $w$. The updated $w$ is going to be used in the next episode in order to predict the value of simulated actions.

Algorithm~\ref{alg:algo} is a formal version of the proposed algorithm.

\begin{algorithm}

\begin{algorithmic}[1]

\vspace{3mm}
\While{the learning process is on}
  \State record $\B{x_t}(p_t)$
    \State $\B{X'} :=\B{X'_{\o}}$ empty $m$-columns matrix
    \For{$\B{x'}$  in  $\Omega(\B{x_t})$}
      \State append  $\B{x'}$  to  $\B{X'}$
    \EndFor
    \State with probability  $1-\epsilon$
    \State \hspace{0.5cm} $\B{x_c}:=\arg\max_{\B{x'} \in \B{X'}} \in f(\B{w},\B{X'})$
    \State with probability  $\epsilon$
    \State \hspace{0.5cm} $\B{x_c}$ := random sample $\B{x'} \in \B{X'}$
    \State append $\B{x_c}$ to $\B{X}$
    \If{$s(\B{x_c})$  !=  $0$}
      \For{$\delta$  in  $1:\Delta$}
        \State $v$ = $d(\gamma, \Delta, \delta, s(\B{x_c}))$
        \State append  $v$  to  $\B{y}$
       \EndFor
      \State $\B{w} := \arg\min_{\B{w}}(l(f(\B{w},\B{X}),\B{y}))$
      \State $\Delta := 0$
    \Else
      \State $\Delta$ += $1$
    \EndIf
\EndWhile

\end{algorithmic}

\caption{}
\label{alg:algo}

\vspace{3mm}
\begin{small} $m$: number of features, $\B{X}=\B{X_{\o}}$: empty $m$-columns matrix, $\B{y}=\B{y_{\o}}$: empty target vector, $\B{w}=\B{w_0}$: random weights for the regressor, $\gamma \in [0,1]$  discount rate, $\Delta = 0$: number of actions, $f()$: regressor function, $p_t(\B{x_c})$: stochastic process, function of the last existing chosen state, $\B{X'}$: $m$-columns simulated states matrix, $\B{x_t}(p_t) \in {\rm I\!R^m}$: current state vector, $\Omega_t(\B{x_t})$: set of possible actions, $\epsilon$: randomness rate, $s(\B{x_c}) \in {\rm I\!R}$: signal value potentially triggered by the modified state \end{small}

\end{algorithm}

\subsection{Remarks} 

\subsubsection{Regressor function} 

One interesting aspect is the role of the regressor function. Contrarily to typical applications, the functions's task is utimately not to predict an action's value with maximum accuracy, even though it is often related to what we want, but rather to give the "real value" to an action. Imagine for instance a game where two intermediary states might lead to both winning and losing. Such states will get a non-tendential value from the function, which is a good thing.

Even if the approximation function's task is this time to unveil the real value of each state, the choice for the right function is still clearly very important. It should notably depend on the complexity of the patterns that we wish the learner to discover, which is often proportional to the complexity of the considered $m$-dimensional states. 

Obviously, if these states approximately form a linear function of the target value, a linear approximation function can be applied. But if the states are strongly affected by stochastic processes, other functions could be considered. 

Typically in this paper, we consider real-world representations as states. In other words, our input could be include images, sounds and texts. In that case, we might consider using neural networks which are often celebrated as a particularly accurate family of algorithms for such data. Using neural networks, we can see our learner as an imitator of how humans would act when facing the task in question. The challenge is then to recognize equivalent underlying states thanks to pattern recognition. Finally note, considering the algorithm described above, that $\B{w}$ would be the set of a neural network's hidden layers. 

\subsubsection{Signal function and discounted values}

The signal function allows to reduce the assumptions on the basic abilities of the learner. While most approximation-based reinforcement learning algorithms suppose the permanent presence of a value-monitoring function, here we only assume the presence of a discrete, time-independent sensor triggering one or more values.

The use of discounted values enables each step to get a value at the end of each episodes. In that process, tuning correctly the $\gamma$ parameter is important. In Figure~\ref{fig:discounts}, we represent a set of actions that to a gain - $s(\B{x_c} = 1$ here, and the discount function that we apply is $\gamma^{\Delta-\delta}s(\B{x_c})$.

\begin{figure}
\begin{center}
\includegraphics[scale=.4]{"discounts"}
\caption{Different discount rates affecting intermediary states}
\label{fig:discounts}
\end{center}
\end{figure}

In the first case $\gamma=0.7$, in the second case $\gamma=0.3$. We see that with a larger discount factor, more emphasis is put on the final result of the set of actions. If relevant, $\gamma$ should be set considering that two intermediary actions can lead to both a gain and a loss.

With this function, triggering a new signal can take time. But the learner still looks for best discounted value according to the current state. Thus, with less assumptions on the monitoring of the real value of the states, we still benefit from something common to RL algorithms: the discovery and use of intermediary objectives.

\section{Behaviour of the algorithm}

We are going to develop a simple insight about why and how the proposed algorithm might lead to optimal long term decisions. For this, we recall the important steps of the algorithm. Firstly, it evaluates the best possible action to take after observing a stochastic state $\B{x_t}(p_t)$. Secondly, it appends the latter or a randomly chosen one to the training features matrix $\B{X}$. If the signal function $s(\B{x^*})$ is triggered, it backwardly assigns values to the states of the episode thanks to the discount function $d(\gamma, \Delta, \delta, s(\B{x^*})$. These values are finally appended to the training target variable $\B{y}$. Then the training set is used as an input for the approximation function $f()$ to find $\B{w} = \arg\min_{\B{w}}(l(f(\B{w},\B{X}),\B{y}))$. In the next episode, the algorithm evaluates a new stochastic state and uses the new $\B{w}$ to make take the next decision.

An important assumption has to be satisfied for the algorithm to take the right decisions. There needs to be patterns in the stochastic states. If it is not the case, the approximation function is unable to optimise the value of $\B{w}$ with time. But it does not make sense to use such algorithm if we know that we are in such setting. By opposition, an application of choice is when we have imperfect representations of the same underlying states, such has pictures of a board game.

Another assumption is that $s(\B{x^*})$ is able to give an objective information about the current state. This is a strong assumption. However, we can imagine a practical solution for this. This signal function could result from a pre-trained supervised approximation function that was able to evaluate with high precision the features’ values triggering each of the possible signal. Another solution, providing perfect information, is manual monitoring at the end of each episode.

Finally, we require a correct tuning of the algorithm’s different elements. The approximation function should be complex enough to recognise the possible patterns in $\B{x_t}(p_t)$. $\Omega(\B{x_t})$ should be well designed. The discount function $d(\gamma, \Delta, \delta, s(\B{x^*}))$ should also be well designed.

With these conditions fulfilled, we firstly expect that, as the training sample is fed after each episode, a better evaluation of the different states and, asymptotically, underlying equivalent states will get the same value. Formally, we anticipate:
$$\B{w} \xrightarrow[n \to \infty]{} \B{w^*}$$
where $n$ is the number of previously observed states and $w^*$ is the optimal set of weights. In most cases, we expect the convergence process to be non-linear.

Evaluating well the value of equivalent states is only a first step to obtain a learner that is useful in practice. To maximise the long-term gains, the algorithm needs to have a relatively balanced experience between gains and losses. First, the tuning of $\gamma$ should allow the regressor to evaluate states faster, via the learner's actions. If necessary, we need to insure that
$d(\gamma, \Delta, \delta, s_{gain}) \simeq d(\gamma, \Delta, \delta, s_{loss})$
if a $\delta$th intermediary action possibly leads to a gain in one episode but also to a loss in another episode. Second, tuning $\epsilon$ well will insure that the learner explores potentially good actions fast enough ; an episode number-dependent value for $\epsilon$ might be desirable. If these final assumptions are insured, we expect:
$$\B{x_c} \xrightarrow[n \to \infty]{} \B{x^*}$$
where $x^*$ is the optimal action for a particular time $t$.

\section{Implementation for a tic-tac-toe robot player}

Tic-tac-toe is a two players game where the players are represented by the symbols X and O in a three by three grid. Each player takes turns marking the spaces with their symbol. The player who places three of his symbol in a horizontal, vertical, or diagonal row wins the game. Note that draw is a possible outcome.

Robots playing this game are a classical application in RL. But this time we want the learner to play according to its \textit{vision} of a board that sequentially drawn with a pen. The information available to the robot corresponds to images transformed into vectors. Ultimately, the learner has to discover that the images shown in Figure~\ref{fig:gameA} represent the same game.

\begin{figure}
\begin{center}
\includegraphics[scale=.14]{"gameA"}
\includegraphics[scale=.35]{"gameA_"}
\caption{After enough games, the learner is able to play the same way after visualizing either of these images}
\label{fig:gameA}
\end{center}
\end{figure}

The images of the board are thus stochastic representations of a finite set of underlying states. The stochastic processes affecting particularly these representations are the variations in the handwriting and the brightness of the environment.

After a sufficient number of games, the robot is able to effectively assign values to the potential actions it might take, as displayed in Figure~.
ADD 150*150 PICTURE WITH PROBAS ON TOP, FOR 100, 1000, 5000 self plays
REVALUE SO LOSS = -1, DRAW=0

\subsection{Particularities of the algorithm's implementation}

The general algorithm presented in the previous section had to be adapted for this particular application, in particular regarding the approximation function, the signal function and the discount function.

The chosen approximation function is an artificial neural network detailed in Figure~\ref{fig:neuralnet}. It outputs real values and is a regressor. The choice for this regressor was driven by the complexity of the relationship existing between the representations of the states and the value assigned to these states. Indeed, this particular architecture proved to capture the infinite number of possible representations of $255168$ possible states, without taking into account rotations and reflections. 

\begin{figure}
\begin{center}
\includegraphics[scale=.25]{"NNview"}
\caption{Other parameters are: the number of iterations = 200, the learning rate = 0.0001, the learning rule = Rmsprop which divides the learning rate for a weight by a running average of the magnitudes of recent gradients for that weight, the threshold under which the validation error change is assumed to be stable = 0.0001, the mini-batch size = 200 observations, and the number of iterations after which training should return when the validation error remains (near) constant = 10.}
\label{fig:neuralnet}
\end{center}
\end{figure}

The stochastic state $\B{x_t}(p_t)$ is resulting from another player's move. The states are the fruit of \textit{rival} actions, either from a human player or from another robot. So in terms of strategy optimization, the learner has to anticipate a possibly defavorable environment during the game and act accordingly. In particular, in this implementation, the learner plays after the rival, so it can win only with three or four moves.

The signal function is set manually. It is triggered when a game ends. If the learner wins, it triggers $1$, $-0.1$ if there is a draw and $-1$ if it loses or if it does an illegal move. Notice that an illegal move corresponds only to playing on top of the other player. The learner here knows where it last played.

In this implementation, the discount function $d(\gamma, \Delta, \delta, s(\B{x_c}))$ has the form
$$\gamma^{\Delta-\delta}s(\B{x_c})$$
More precisely, I picked $\gamma=0.3$ so we get neutral values, close to $0$, for early moves regardless of the game's outcome. The possible discounted values for moves leading to a win thus are: $1$, $0.3$, $0.09$ and $0.081$. For a draw: $-0.1$, $-0.01$, $-0.001$ and $-0.0001$. For a loss: $-1$, $-0.3$, $-0.09$ and $-0.081$. When an illegal move happens, there is no discount computation and this move gets the value $-1$.

Finally, I implemented an adaptive randomness factor. It has the very \textit{ad hoc} form 
$$\epsilon = 30000/log_{10}(S)^6,$$
where $S$ is the size of the data in bytes. I chose this formula so it equals 99\% for a data corresponding to 10000 games which is how many games I train the learner with. The other robot is penalized with $\epsilon = 120000/log_{10}(S)^6$ so our learner has time to learn how to win in the beginning. This parameter has to be set sensibly for an optimized training to get a perfect "player". However we will not study this aspect further here.

\subsection{Necessary adjustments}

A few adjustments were to be made. First, notice that the learning phase is done against a Q-learning robot. After each of the players' move, an image representing the current state of the board is generated. This is image is then reduced and appended to the training set, as shown in Figure~\ref{fig:transition}.

\begin{figure}
\begin{center}
\includegraphics[scale=.4]{"discounts"}
\caption{Different discount rates affecting intermediary states}
\label{fig:transition}
\end{center}
\end{figure}

Second, our learner had the disadvantage of receiving imperfect information about the game compared to the other robot, knowing exactly where the moves are made. To compensate for this, our learner after each move rotates the game three times and appends the learned values accordingly.

Third, as we do not have an objective sensor at our disposal, the signal recieved by the learner is the same as the signal coming from the oponent robot. It is an objective information indicating the final state of game when it happens.

Third, the learning process, via neural networks, does not happen after each game but at regular intervals. This is purely for computational constraints. However, this does not affect in fine the performance of the learner, which is able to win up to 90\% of the last phases' games.

\subsection{Performance analysis}

This section aims at assessing that the proposed algorithm leads to winning strategies.

The following plots show the evolution the learner's performance during the learning phase. We also study how the proportions of wins and losses (and draws by deduction) vary according to the levels of $\epsilon$ for each type of robot. Recall that the robot updates its parameters every 100 games, and can potentially improve at this moment. 

The green line corresponds to the proportion of won games for the last 100 games The red line corresponds to the proportion of lost games for the last 100 games.

INCLUDE R PLOTS

We see that a more exploratory learner improves its winning rate faster but is then relatively limited towards the last phases.

A more exploratory opponent helps the learner to win and thus discover what are the winning strategies. However, making the opponent too exploratory would limit the performance of the learner when playing against a trained human.

Notice that a learner trained against a random opponent is able, after 100 games, to play easy moves, typically the last move to win, against a human player.

The last plot shows the average number of moves before winning for the last 100 games. 

PLOT AVG MOVES

The learner can only win in three or four moves. Against a random player, we that the learner has an average number of moves to win closer to three with more games.

\section{Conclusion}

The proposed algorithm behaved as expected with its tic-tac-toe implementation. More work has to be done concerning the proof of convergence for the regressor function and the proof that asymptotically the behaviour of the learner is optimal.

Now, we can think of more useful applications than this overkill tic-tac-toe player. For instance, a learner of this type, installed on augmented reality glasses could help people achieving precise tasks in a more optimal way. It could also be useful in online marketing to automatically identify set of actions leading to better sales, such as the sequence of clicks of customers. We also can imagine learning (thanks to vision) the way natural organisms evolve in their environment to build very adaptive machines.

\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

\begin{thebibliography}{99}

\bibitem{c1} Sutton and Barto, Reinforcement learning: An introduction, Vol. 2. No. 1, MIT press, 2012

\bibitem{c2} $http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching_files/FA.pdf$

\bibitem{c3} https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf

\bibitem{c4} http://gaips.inesc-id.pt/~fmelo/pub/melo08icml.pdf

\bibitem{c5} Human-level control through deep reinforcement learning

\bibitem{c6} Munos et al. (2016), Safe and efficient off-policy reinforcement learning, arXiv:1606.02647v1 [cs.LG]

\bibitem{c7} Seijen et al. (2015), True Online Temporal-Difference Learning, arXiv:1512.04087v1 [cs.AI]

\end{thebibliography}

\end{document}