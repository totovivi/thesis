\documentclass[letterpaper, 10.5 pt, conference]{ieeeconf}
\IEEEoverridecommandlockouts
\overrideIEEEmargins

\usepackage{bm}
\usepackage{bbm}
\usepackage{algorithm,algpseudocode}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\newcommand{\B}[1]{\mathbf{#1}}

\title{\LARGE \bf
Automated consistent course of actions in a stochastic environment
}

\author{Thomas Vicente$^{1}$% <-this % stops a space
\thanks{*Candidate for the Master in Data Science at the Barcelona Graduate School of Economics.}% <-this % stops a space
}

\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}

\begin{abstract}

I propose an algorithm allowing the automated learning of the best course of actions in a stochastic environment. It notably includes a signal function that should allow more flexible implementations than current approximation-based reinforcement learning algorithms. This function triggers the computation of discounted action values for each learning episode. The paper contains an application, a tic-tac-toe robot player recognizing underlying games from photographs via a neural network.

\end{abstract}

\section*{Acknowledgment}

I would like to thank my advisor, Professor GÃ¡bor Lugosi, for his help and for making himself available. I would also like to thank Gergely Neu, post-doctoral fellow at the Artificial Intelligence Research Group from the Pompeu Fabra University, for sharing his knowledge on reinforcement learning.

\section{Introduction}

A complete exercise would be to build an autonomous entity that is progressively able to take long term-oriented decisions based on information coming from real world sensors. This entity needs two abilities: learning from its own actions and recognizing useful patterns in the current state. The first ability typically corresponds to algorithms from the reinforcement learning (RL) field. The second ability requires regressor or classifier function that are able to handle the complexity of stochastic state's patterns. 

\subsection{Learning by action}

RL emerges easily from other Machine Learning techniques as the training data is generated thanks to the actions of the learning entity. Thus, the \textit{learner} uses training information that evaluates the actions taken rather than instructs by giving correct actions are faced repeatedly with a choice among $n$ different options, or actions. Such setting has its roots in the \textit{n-armed bandit problem} where after each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the selected action. The objective is to maximize the expected total reward over all the episodes. The learner evolves in a particular state and records what actions lead to a maximal gain. In most cases the goal is to maximize a long term gain by taking a correct course of actions. These policies can be more or less explicit. I our case, the proposed algorithm would be qualified as \textit{off-policy}.

An RL learner is thus particularly dependent on a state signal that succeeds in retaining all relevant information, a \textit{Markov} state. For example, a checkers position (the current configuration of all the pieces on the board) would serve as a Markov state because it summarizes everything important about the complete sequence of positions that led to it. Much of the information about the sequence is lost, but all that really matters for the future of the game is retained. 

A reinforcement learning task that satisfies the Markov property is called a Markov Decision Process (MDP). If the state and action spaces are finite, it is possible the MDP framework notably formalizes the probability that a state occurs given a previous state an action, and the probability of reward after the new state occurred.

\subsection{Pattern recognition}

Equipping the learner with an approximation function enables it to evaluate an infinite set of states but also a finite set of underlying states having an infinite set of representations. In Figure~\ref{fig:surf}, I illustrate the fact that humans are able to recognize the same underlying situation and take the correct course of actions.

\begin{figure}
\begin{center}
\includegraphics[scale=.35]{"surf"}
\caption{Waves are a stochastic process but, with training, we are able to act consistently for a new wave}
\label{fig:surf}
\end{center}
\end{figure}

In a similar manner, we would like our learner to overcome the imperfect information it receives and learn how to behave in the long term to maximize its gain. This is made possible notably by regressor and classifier functions that take particularly well advantage of naturally generated data such as images and sounds.

A modified version of the classical tic-tac-toe robot will provide a good example for the principle we seek to study.

\section{Literature review}

The proposed algorithm allows to optimize a course of actions in finite and infinite spaces. But it has especially been thought to act based on imperfectly sensed states as described above.

A this is where it differs for example from the State-Action-Reward-State-Action algorithm (SARSA). It uses approximations to update the value of a whole set of actions, a policy. SARSA is well suited when the value of each action can be monitored. It notably update the parameters of the approximation function after each movement thanks to a change in value from $t-1$ to $t$, known as a temporal difference. The update includes a gradient term, compute from the difference between the state's value approximation and the real state's value.

The Deep Q networks (DQN) are close to what I propose. This consists in an off-policy algorithm using neural network approximations of the states' value to decide on an action. And as the learner makes better moves, the neural network is fed with more useful information. And thanks to the so called "Experience replay", when training the network, random mini-batches from the replay memory are used instead of the most recent transition. Such efficient implementation makes the DQN very adaptive to different tasks, as demonstrated for the learning of multiple arcade games while using the same network architecture. 

Hopefully the proposed algorithm offers even more flexibility than the evoked algorithms. That would be thanks to removing the assumption that the state's values are permanently monitored during the training phase. This is possible by the implementation of a \textit{signal function}. Its role is to detect a preset objective signal thanks to a deterministic function or a well tuned sensor. The aim is notably to allow the algorithm to be practical when evaluating non-idealized states - unlike video game screen shots - involving stochastic perturbations - such as real world pictures or videos.

\section{Algorithm}

\subsection{Description}

I am going to describe chronologically what happens during an episode of the algorithm in action. Note that all the objects containing $X$ or $x$ in their names are $m$-dimensional objects, where $m$ is the number of features representing the considered states.

The learner observes a new stochastic state $\B{x_t(p_t)}$ - note that this stochastic state can be affected by previous moves from our learner. It then simulates actions within the finite set of possible actions $\Omega(\B{x_t})$. $\Omega(\B{x_t})$ can be set deterministically. For instance, in the context of a game, there is a well determined set of actions that a human can tune in advance. It also can be a subset of an infinite set of actions. For instance, a mobile robot can explore the real 3D world in almost infinite ways. In that case, the set of possible actions has to be restricted somehow, possibly by the time allowed to go in particular direction.

All of these simulated actions are stored in a temporary matrix $\B{X'}$. Each observation in this matrix is going to be evaluated by the a approximation function $f(\B{w^*},\B{X'})$. This function can be a regressor or a classifier which weights $w^*$ are randomly initialized before the first episode. It is going to return approximations of the current state's value. This value is proportional to the learner's closeness to its objective. Among the simulated states, we will then return with probability $1-\epsilon$ the state which has the highest approximated value, $\B{x^*}$. With probability $\epsilon$, we will randomly pick a state from $\B{X'}$. If both cases, we append the returned state to the training set $\B{X}$.

The learner possesses a function $s(\B{x^*})$. It returns different values when a particular signal in the state generated by the learner's last move, $\B{x^*}$, is sensed. The value is positive if a \textit{gain} is sensed, and negative value if a \textit{loss} is sensed. At least one value should be potentially triggered. A typical example for $[gain;loss]$ would be respectively $[1;-1]$. While this signal is off, the learner counts the number of actions since the signal was last triggered. When the signal is triggered, the current episode is over. Each action of the episode receives a value given by the discount function $d(\gamma, \Delta, \delta, s(x^*))$ where $\gamma$ is a discount rate, $\Delta$ is the number of actions in the episode and $\delta$ is the index of the action in the episode. For instance, the last action in the episode has $\delta=\Delta$. The function should insure that actions that are closer in time from the triggered signal get values that are closer from $s(\B{x^*})$. Each value of the episode are then appended to the training target vector $\B{y}$. 

We then get a training sample on which are going to apply our approximation function $f(\B{w}, \B{X})$ which this time minimizes the loss $l(f(\B{w}, \B{X}),\B{y})$. From this process, we are going to retain set of parameters corresponding to the minimal loss to update $w^*$. The updated $w^*$ is going to be used in the next episode in order to predict the value of simulated actions.

Algorithm~\ref{alg:algo} is a formal version of the proposed algorithm.

\begin{algorithm}
\caption{Approximation-based reinforcement learning with signal function}
\label{alg:algo}
\begin{algorithmic}[1]

\State $m$ : number of features
\State $\B{X}=\B{X_{\o}}$: empty $m$-columns matrix
\State $\B{y}=\B{y_{\o}}$: empty target vector
\State $\B{w^*}=\B{w_0}$: random weights for the regressor
\State $\gamma \in [0,1]$  discount rate
\State $\Delta = 0$ : number of actions
\State $f()$ : regressor function
\State $p_t(\B{x^*})$: stochastic process, function of the last existing chosen state
\State $\B{X'}$: $m$-columns simulated states matrix
\State $\B{x_t(p_t)} \in {\rm I\!R^m}$: current state vector
\State $\Omega_t(\B{x_t})$ : set of possible actions
\State $\epsilon$: randomness rate
\State $s(\B{x^*}) \in {\rm I\!R}$: signal value potentially triggered by the modified state

\While{the learning process is on}
  \State record $\B{x_t(p_t)}$
    \State $\B{X'} :=\B{X'_{\o}}$ empty $m$-columns matrix
    \For{$\B{x'}$  in  $\Omega(\B{x_t})$}
      \State append  $\B{x'}$  to  $\B{X'}$
    \EndFor
    \State with probability  $1-\epsilon$
    \State \hspace{0.5cm} $\B{x^*}:=\arg\max_{\B{x'} \in \B{X'}} \in f(\B{w^*},\B{X'})$
    \State with probability  $\epsilon$
    \State \hspace{0.5cm} $\B{x^*}$ := random sample $\B{x'} \in \B{X'}$
    \State append $\B{x^*}$ to $\B{X}$
    \If{$s(\B{x^*})$  !=  $0$}
      \For{$\delta$  in  $1:\Delta$}
        \State $v$ = $d(\gamma, \Delta, \delta, s(\B{x^*}))$
        \State append  $v$  to  $\B{y}$
       \EndFor
      \State $\B{w^*} := \arg\min_{\B{w}}(l(f(\B{w},\B{X}),\B{y}))$
      \State $\Delta := 0$
    \Else
      \State $\Delta$ += $1$
    \EndIf
\EndWhile

\end{algorithmic}
\end{algorithm}

\subsection{Remarks} 

\subsubsection{Regressor function} 

One interesting aspect is the role of the regressor function. Contrarily to typical applications, the functions's task is utimately not to predict an action's value with maximum accuracy, even though it is often related to what we want, but rather to give the "real value" to an action. Imagine for instance a game where two intermediary states might lead to both winning and losing. Such states will get a non-tendential value from the function, which is a good thing.

Even if the approximation function's task is this time to unveil the real value of each state, the choice for the right function is still clearly very important. It should notably depend on the complexity of the patterns that we wish the learner to discover, which is often proportional to the complexity of the considered $m$-dimensional states. 

Obviously, if these states approximately form a linear function of the target value, a linear approximation function can be applied. But if the states are strongly affected by stochastic processes, other functions could be considered. 

Typically in this paper, we consider real-world representations as states. In other words, our input could be include images, sounds and texts. In that case, we might consider using neural networks which are often celebrated as a particularly accurate family of algorithms for such data. Using neural networks, we can see our learner as an imitator of how humans would act when facing the task in question. The challenge is then to recognize equivalent underlying states thanks to pattern recognition. Finally note, considering the algorithm described above, that $\B{w}$ would be the set of a neural network's hidden layers. 

\subsubsection{Signal function and discounted values}

The signal function allows to reduce the assumptions on the basic abilities of the learner. While most approximation-based reinforcement learning algorithms suppose the permanent presence of a value-monitoring function, here we only assume the presence of a discrete, time-independent sensor triggering one or more values.

The use of discounted values enables each step to get a value at the end of each episodes. In that process, tuning correctly the $\gamma$ parameter is important. In Figure~\ref{fig:discounts}, we represent a set of actions that to a gain - $s(\B{x^*} = 1$ here, and the discount function that we apply is $\gamma^{\Delta-\delta}s(\B{x^*})$.

\begin{figure}
\begin{center}
\includegraphics[scale=.5]{"discounts"}
\caption{Different discount rates affecting intermediary states}
\label{fig:discounts}
\end{center}
\end{figure}

In the first case $\gamma=0.7$, in the second case $\gamma=0.3$. We see that with a larger discount factor, more emphasis is put on the final result of the set of actions. If relevant, $\gamma$ should be set considering that two intermediary actions can lead to both a gain and a loss.

With this function, triggering a new signal can take time. But the learner still looks for best discounted value according to the current state. Thus, with less assumptions on the monitoring of the real value of the states, we still benefit from something common to RL algorithms: the discovery and use of intermediary objectives.

\section{Behaviour of the algorithm}

\subsection{Convergent estimates for equivalent underlying states}

The convergence of the algorithm's evaluations is closely linked to the performance of the regressor function at extracting underlying patterns between states and values, assuming however that this relationship exists.

Let's consider a finite set of underlying states. If the conditions are favorable, we expect overtime, as the training sample is fed after each episode, a better evaluation of the different states and, asymptotically, underlying equivalent states will get the same value. Under another perspective, we expect $w^*$ to converge as $X$ grows. 

An interesting point is the expected non-linearity of this process. Indeed, as long as the learner did not discover all gain-yielding states, the regressor's parameters do not permanently stabilize. Once all states have been uncovered, this convergence happens.

If the set of underlying states is infinite, the regressor's parameters can stabilize only if there exist patterns between the states and the value. An example of this situation is a vision-equipped learner whose task is to catch the same ball in different environments.

\subsection{Long term gain maximization}

Evaluating well the value of equivalent states is only a first step to obtain a learner that is useful in practice. To behave in a long term-maximizing way, the algorithm needs to have a relatively balanced experience between gains and losses. 

First, tuning $\gamma$ in a proper way will help to allow the regressor to evaluate states well faster, via the learner's actions. If possible, a good way to tune this parameter is to make such that
$$d(\gamma, \Delta, \delta, s_{gain}) \simeq d(\gamma, \Delta, \delta, s_{loss})$$
for the $\delta$th intermediary action that led to a gain in one episode but also to a loss in another episode.

Second, tuning $\epsilon$ well will insure that the learner explores potentially good actions fast enough. A number of episodes-dependent $\epsilon$ might be desirable in that regard.

\section{Implementaion for a "real-world" tic-tac-toe game}

The tic-tac-toe game is a game with initially nine positions organised as a three by three square. After the first player played the first move, the second player is left with eight available moves. This is until one player won or once the whole board is covered, yielding a draw. To win, a player must align his moves on the same line or on the same column, or by filling one of the board's diagonal.

The tic-tac-toe game is a RL classical application, but this time the learner has to play according to his "vision". Notably, when playing against a human, the information available to the robot corresponds to an image of the type:

TWO IMAGES
caption: after enough games, the learner should be able to play the same way after visualizing either of these images

This is thus a "stochastic representation" from a finite set of states. The stochastic processes affecting this representation are notably the varying handwrittings and the environment's brightness.

After a sufficient number of games, the robot is able to effectively assign values to the potential actions it might take:
ADD 150*150 PICTURE WITH PROBAS ON TOP, FOR 100, 1000, 5000 selfplays
REVALUE SO LOSS = -1, DRAW=0

\subsection{Particularities of the implementation}

The chosen regressor is an artificial neural network detailed in Figure~\ref{fig:neuralnet}. I chose this type of regressor as I believe it supports the complex relationship between the pixels representing a state and the value of this state. A sufficiently complex structure is thus able to handle the very large number of ways to represent a game combined with the many game combinations. In this implementation, the network is has the following form:

\begin{figure}
\begin{center}
\includegraphics[scale=.25]{"NNview"}
\caption{Other parameters are: the number of iterations = 200, the learning rate = 0.0001, the learning rule = Rmsprop which divides the learning rate for a weight by a running average of the magnitudes of recent gradients for that weight, the threshold under which the validation error change is assumed to be stable = 0.0001, the mini-batch size = 200 observations, and the number of iterations after which training should return when the validation error remains (near) constant = 10.}
\label{fig:neuralnet}
\end{center}
\end{figure}

The stochastic state $x_t$ is resulting from another player's move represented on an image as seen above. In terms of strategy optimization, here the learner would have to think ahead. The states here are the fruit of *rival* actions, either from a human player or from another robot. And in this implmentation, the learner plays after the rival, so it can win either in three or four moves.

The signal function is set by a human. It triggers the values $1$ if the learner wins, $-0.1$ if there is a draw and $-1$ if it loses or if it does an illegal move. Notice that an illegal move corresponds only to playing on top of the other player. The learner here knows where it last played.

In this implementation, the discount function $d(\gamma, \Delta, \delta, s(x^*))$ has the form
$$\gamma^{\Delta-\delta}s(x^*)$$
More precisely, I picked $\gamma=0.3$ as it yields the same value for an early move. The discounted value for moves leading to a win thus are: 1, 0.3, 0.09 and 0.081. For a draw: -0.1, -0.01, -0.001 and -0.0001. For a loss: -1, -0.3, -0.09 and -0.081. For an illegal move, only the signal value is given to the last move.

I implemented an adaptive randomness factor. It has the very *ad hoc* form 
$$\epsilon = 50000/log_{10}(S)^6,$$
where $S$ is the size of the data in bytes. I chose this formula so it equals 99\% for a data corresponding to 10000 games. The other learner is penalized and has $\epsilon = 100000/log_{10}(S)^6$ so our learner has time to learn how to win.

\subsection{Necessary adjustments}

A few adjustments were to be made. First, notice that the learning phase is done against a Q-learning robot. After each of the players' move, an image representing the current state of the board is generated. This is image is then reduced and appended to the training set.
SHOW LARGE IMAGE -> SMALL IMAGE transition

Second, our learner had the disadvantage of receiving imperfect information about the game compared to the other robot, knowing exactly where the moves are made. To compensate for this, our learner after each move rotates the game three times and appends the learned values accordingly.

Third, as we do not have an objective sensor at our disposal, the signal recieved by the learner is the same as the signal coming from the oponent robot. It is an objective information indicating the final state of game when it happens.

Third, the learning process, via neural networks, does not happen after each game but at regular intervals. This is purely for computational constraints. However, this does not affect in fine the performance of the learner, which is able to win up to 90\% of the last phases' games.

\subsection{Performance analysis}

This section aims at assessing that the proposed algorithm leads to winning strategies.

The following plots show the evolution the learner's performance during the learning phase. We also study how the proportions of wins and losses (and draws by deduction) vary according to the levels of $\epsilon$ for each type of robot. Recall that the robot updates its parameters every 100 games, and can potentially improve at this moment. 

The green line corresponds to the proportion of won games for the last 100 games The red line corresponds to the proportion of lost games for the last 100 games.

INCLUDE R PLOTS

We see that a more exploratory learner improves its winning rate faster but is then relatively limited towards the last phases.

A more exploratory opponent helps the learner to win and thus discover what are the winning strategies. However, making the opponent too exploratory would limit the performance of the learner when playing against a trained human.

INCLUDE ADAPTATIVE LEARNING RATE?

Notice that a learner trained against a random opponent is able, after 100 games, to play easy moves, typically the last move to win, against a human player.

The last plot shows the average number of moves before winning for the last 100 games. 

PLOT AVG MOVES

The learner can only win in three or four moves. Against a random player, we that the learner has an average number of moves to win closer to three with more games.

\subsection{Improvements}

Convolutional layer

Detect all illegal moves

\section{Conclusion}

The proposed algorithm behaved as expected with its tic-tac-toe implementation. More work has to be done concerning the proof of convergence for the regressor function and the proof that asymptotically the behaviour of the learner is optimal.

Now, we can think of more useful applications than this overkill tic-tac-toe player. For instance, a learner of this type, installed on augmented reality glasses could help people achieving precise tasks in a more optimal way. It could also be useful in online marketing to automatically identify set of actions leading to better sales, such as the sequence of clicks of customers. We also can imagine learning (thanks to vision) the way natural organisms evolve in their environment to build very adaptive machines.

\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

\begin{thebibliography}{99}

\bibitem{c1} Sutton and Barto, Reinforcement learning: An introduction, Vol. 2. No. 1, MIT press, 2012

\bibitem{c2} $http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching_files/FA.pdf$

\bibitem{c3} https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf

\bibitem{c4} http://gaips.inesc-id.pt/~fmelo/pub/melo08icml.pdf

\bibitem{c5} Human-level control through deep reinforcement learning

\end{thebibliography}

\end{document}